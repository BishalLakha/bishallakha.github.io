<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://bishallakha.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bishallakha.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-26T18:12:20+00:00</updated><id>https://bishallakha.github.io/feed.xml</id><title type="html">Bishal Lakha</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Causal Explanation for Graph Neural Network</title><link href="https://bishallakha.github.io/blog/2024/causal-explanation-for-graph/" rel="alternate" type="text/html" title="Causal Explanation for Graph Neural Network"/><published>2024-12-21T21:01:00+00:00</published><updated>2024-12-21T21:01:00+00:00</updated><id>https://bishallakha.github.io/blog/2024/causal-explanation-for-graph</id><content type="html" xml:base="https://bishallakha.github.io/blog/2024/causal-explanation-for-graph/"><![CDATA[<p>Coming soon !</p>]]></content><author><name></name></author><category term="essay"/><category term="ai"/><summary type="html"><![CDATA[Study Notes]]></summary></entry><entry><title type="html">Graph Diffusion Model and Its Applications</title><link href="https://bishallakha.github.io/blog/2024/graph-diffusion-model/" rel="alternate" type="text/html" title="Graph Diffusion Model and Its Applications"/><published>2024-04-09T21:01:00+00:00</published><updated>2024-04-09T21:01:00+00:00</updated><id>https://bishallakha.github.io/blog/2024/graph-diffusion-model</id><content type="html" xml:base="https://bishallakha.github.io/blog/2024/graph-diffusion-model/"><![CDATA[<h2 id="1-introduction">1 Introduction</h2> <p>The phenomenal performance of AI systems for image and video generation such as DALLE [1], Stable Diffusion [2], and Sora [3] has captured the imagination of people and triggered the interest of many [4, 5, 6]. Underlying these AI systems is a deep learning algorithm called diffusion models or denoising diffusion probabilistic models (DDPM). The diffusion model, first proposed by Sohl-Dickstein et al. [7], and later improved by Ho et al. [8], enabling practical use cases, is a physics-inspired generative model. In physics, diffusion refers to the process by which particles move from regions of higher concentration to regions of lower concentration to reach an equilibrium, mathematically captured by Fick’s laws [9]. These laws describe the rate of diffusion by taking into account the concentration gradient between two points [10]. High-dimensional data also behaves similarly to the randomly moving particles as they seek an optimal distribution or representation, making diffusion suitable for generative tasks [10].</p> <p>The diffusion model involves two main processes: the forward process (diffusion) and the reverse process (denoising), as illustrated in Fig. 1. The forward process requires gradually degrading the data, such as an image, through a multi-step noise application that converts it into a sample from a Gaussian distribution, discussed in detail in Section 1.1. Conversely, the reverse process, detailed in Section 1.2, involves training a deep neural network to reverse the noising steps, enabling the generation of new data from Gaussian-distributed samples [11]. Unlike other generative models like Generative Adversarial Networks (GAN) [12], diffusion models are easy to train and can scale well on parallel hardware, making them quite suitable for large-scale datasets [11]. They also avoid the problem of instability during training and generate better results in comparison to those algorithms, leading to their increased adoption in research and applications [13].</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blogs/graph_diffusion/diffusion-480.webp 480w, /assets/img/blogs/graph_diffusion/diffusion-800.webp 800w, /assets/img/blogs/graph_diffusion/diffusion-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/graph_diffusion/diffusion.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Illustration of forward and reverse process on an Image [14] </div> <h3 id="11-forward-process">1.1 Forward Process</h3> <p>The forward process incrementally introduces noise into the data, transforming a clean data point \(x_0\) into a series of increasingly noisy latent variables \(x_1, x_2, \ldots, x_T\) through a Markov chain defined as:</p> \[q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)\] <p>Here, \(\beta_t\) modulates the noise level, and \(\mathcal{N}\) denotes a Gaussian distribution. The entirety of the forward process can be expressed as:</p> \[q(x_{1:T}|x_0) = \prod_{t=1}^T q(x_t|x_{t-1})\] <p>A notable aspect of this process is the direct sampling of \(x_t\) at any noise level using:</p> \[q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I)\] <p>where:</p> \[\bar{\alpha}_t = \prod_{s=1}^t (1 - \beta_s)\] <hr/> <h4 id="algorithm-1-training-a-ddpm-15">Algorithm 1: Training a DDPM [15]</h4> <ol> <li>For every image \(x_0\) in your training dataset:</li> <li><strong>Repeat</strong>: <ul> <li>Pick a random time step \(t \sim \text{Uniform}[1, T]\).</li> <li>Draw a sample \(x_t \sim \mathcal{N}(x_t|\sqrt{\alpha_t}x_0, (1-\alpha_t)I)\), i.e., \(x_t = \alpha_t x_0 + \sqrt{1-\alpha_t}z, \, z \sim \mathcal{N}(0, I)\)</li> <li>Take a gradient descent step on: \(\nabla_\phi || \hat{x}_\phi(x_t) - x_0 ||^2\)</li> </ul> </li> <li><strong>Until convergence</strong></li> </ol> <p>You can do this in batches, just like how you train any other neural network. Note that, here, you are training one denoising network \(\hat{x}_\phi\) for all noisy conditions.</p> <hr/> <h3 id="12-reverse-process">1.2 Reverse Process</h3> <p>The goal of the reverse process of the DDPM is to reconstruct the clean data by denoising, predicting \(x_{t-1}\) from $x_t$ at each step using the equation:</p> \[p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))\] <p>where \(\mu_\theta(x_t, t)\) and \(\Sigma_\theta(x_t, t)\) are functions modeled by a neural network \(\theta\), determining the mean and covariance of the Gaussian distribution at each reverse step. The entire reverse process can be described as:</p> \[p_\theta(x_{0:T}) = p(x_T) \prod_{t=T}^1 p_\theta(x_{t-1}|x_t)\] <p>This process begins with the assumption that the final noisy data point, \(x_T\), follows a Gaussian distribution, typically centered around zero with identity covariance. The subsequent denoising steps iteratively estimate the less noisy preceding states until the original data point \(x_0\) is recovered.</p> <hr/> <h5 id="algorithm-2-inference-on-a-ddpm-15">Algorithm 2: Inference on a DDPM [15]</h5> <ol> <li>You give a white noise vector \(x_T \sim \mathcal{N}(0, I)\).</li> <li><strong>For</strong> \(t = T, T-1, \ldots, 1\) <strong>do</strong>: <ol> <li>Calculate \(\hat{x}_\theta(x_t)\) using our trained denoiser.</li> <li>Update according to: \(x_{t-1} = \frac{(1-\alpha_{t-1})\sqrt{\alpha_t}}{1-\alpha_t} x_t + \frac{(1-\alpha_t)\sqrt{\alpha_{t-1}}}{1-\alpha_t} \hat{x}_\theta(x_t) + \sigma_q(t)z, \, z \sim \mathcal{N}(0, I)\)</li> </ol> </li> <li><strong>End For</strong></li> </ol> <hr/> <h3 id="13-training-objective">1.3 Training Objective</h3> <p>The training objective for the diffusion model is to maximize the variational lower bound (ELBO) on the log-likelihood expressed as:</p> <p>\(\text{ELBO}_{\phi, \theta}(x) = \mathbb{E}_{q_\phi(x_{1:T}|x_0)} [\log p_\theta(x_0|x_1)] \text{D}_{\text{KL}}(q_\phi(x_T|x_0) || p(x_T)) \sum_{t=2}^T \mathbb{E}_{q_\phi(x_t|x_0)} \left[ \text{D}_{\text{KL}} \left( q_\phi(x_{t-1}|x_t, x_0) || p_\theta(x_{t-1}|x_t) \right) \right]\) The first component of the equation, the reconstruction term, measures how well the model \(p_\theta\) can reconstruct the initial data point \(x_0\) from the latent variable \(x_1\), using the log-likelihood \(p_\theta(x_0|x_1)\). The second component involves the KL divergence, which measures the difference between the distribution \(q_\phi(x_T|x_0)\) and the prior distribution \(p(x_T)\). The last component, the consistency term, sums the KL divergences across transitions for \(t = 2\) to \(T\), measuring the alignment between the forward transition modeled by \(q_\phi(x_{t-1}|x_t, x_0)\) and the reverse transition \(p_\theta(x_{t-1}|x_t)\). The ELBO can be further simplified to get a loss function (see [15]):</p> \[\theta^* = \underset{\theta}{\text{arg min}} \sum_{t=1}^T \frac{1}{2\sigma^2(t)} \frac{(1-\alpha_t)^2 \alpha_{t-1}}{(1-\alpha_t)^2} \mathbb{E}_{q(x_t|x_0)} \left[ \| \hat{x}_\theta(x_t) - x_0 \|^2 \right].\] <p>Algorithm 1 and 2 summarize the training and inference procedures for the diffusion model.</p> <h3 id="14-guided-diffusion">1.4 Guided Diffusion</h3> <p>In many applications, like text prompt-based image generation, generating samples from a conditional distribution \(p(x \mid c)\) is desired, where \(c\) is the conditioning variable. However, the diffusion model might assign insufficient weight to these conditions. To address this, additional pressure called “guidance” is applied, resulting in guided diffusion [11].</p> <p>There are primarily two types of guidance: <strong>classifier guidance</strong> and <strong>classifier-free guidance</strong>.</p> <ol> <li> <p><strong>Classifier Guidance</strong>: In this approach, a separate classifier model \(p(c \mid x)\) is utilized along with a diffusion model to drive the sample generation towards desired characteristics defined by the conditional label \(c\) [16].</p> </li> <li> <p><strong>Classifier-Free Guidance</strong>: In this approach, instead of employing a separate classifier for guidance, conditional and unconditional diffusion processes are jointly trained to achieve the desired outcomes [17].</p> </li> </ol> <hr/> <h2 id="2-graph-diffusion-models">2. Graph Diffusion Models</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blogs/graph_diffusion/graph_diffusion-480.webp 480w, /assets/img/blogs/graph_diffusion/graph_diffusion-800.webp 800w, /assets/img/blogs/graph_diffusion/graph_diffusion-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/graph_diffusion/graph_diffusion.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: Forward and reverse process on graph data [18] </div> <p>A graph $G$, defined as a pair \(G = (V, E)\), is a fundamental structure in mathematics and computer science used to model relationships and interaction between the objects. Here $V$ is a set of vertices and \(E\) is a set of edges, each connecting a pair of vertices. Graphs can be used to model different complex and interconnected data like social networks, recommendation systems, biological networks, and other areas, which makes adaptation of generative models like diffusion models for graphs quite crucial.</p> <p>As illustrated in Fig 2, DDPM on graphs involves forward and reverse processes where the primary focus is on creating a transition kernel for the Markov chain [18]. Various methods have been proposed to achieve that. Haefeli et al. [19] proposed using discrete noise for the forward Markov process instead of using continuous Gaussian perturbations, which ensured that for every intermediate step, the graph remained discrete, resulting in better and faster sample generation. We discuss the forward and reverse process proposed by [19] in Section 2.1 and 2.2.</p> <h3 id="21-forward-process">2.1 Forward Process</h3> <p>The diffusion model generates a sequence from an initial simple graph \(A_0\) (adjacency matrix representing the graph) to white noise $A_T$ through a series of increasingly noisier graphs \(A_1, A_2, \ldots, A_T\). Both \(A_0\) and \(A_T\) are adjacency matrices where \(A_0\) is a sample from the dataset and \(A_T\) is an Erdős–Rényi [20] random graph.</p> <p>Each element \(a_{ij}^t\) of the adjacency matrix between nodes \(i\) and \(j\) is encoded as a one-hot vector and transformed by a double stochastic matrix \(Q_t\) defined as:</p> \[Q_t = \begin{bmatrix} 1 - \beta_t &amp; \beta_t \\ \beta_t &amp; 1 - \beta_t \end{bmatrix}\] <p>Here, \(\beta_t\) indicates the probability of the edge state not changing. This formulation allows for direct sampling at any timestep \(t\) independently for each edge and non-edge, facilitating the simplification of the sampling process without relying on previous timesteps.</p> <h3 id="22-reverse-process">2.2 Reverse Process</h3> <p>The reverse process aims to recover the original graph from the noise. The reverse transition is denoted as \(q(A_{t-1} \mid A_t, A_0)\) and is crucial for training to learn how to denoise the graphs. The reverse transition probabilities are derived from the forward probabilities with a dependence on the initial graph \(A_0\), ensuring accurate regeneration of the graph, as given by:</p> \[q(A_{t-1} \mid A_t, A_0) = \frac{q(A_t \mid A_{t-1}) q(A_{t-1} \mid A_0)}{q(A_t \mid A_0)}.\] <h3 id="23-latent-diffusion">2.3 Latent Diffusion</h3> <p>Diffusion over discrete graph space can suffer from different issues, primarily high modeling complexity, complex relational information leading to limited semantic learning, and consequently poor performance [21]. Instead, using a latent space can improve efficiency with faster sampling and produce better samples by producing smoother representation [21, 22, 23].</p> <p>This is generally achieved by first training a variational graph autoencoder (VGAE) to capture topological information and then applying diffusion with some conditioning on its latent space to enhance the representation, and finally using the decoder to generate the graph [21, 22, 23, 24].</p> <hr/> <h2 id="3-applications">3 Applications</h2> <p>Graph diffusion models have demonstrated substantial efficacy across a diverse range of fields like biology, chemistry, physics, computer science, and others. In this section, we will explore several applications of these models, highlighting their role and impact in research and industry.</p> <h3 id="31-molecule-and-protein-design">3.1 Molecule and Protein Design</h3> <p>Diffusion models can generate novel molecules by learning the distribution of existing molecular graphs. This is particularly useful for discovering new drugs with desired properties. For example, <strong>DiffHopp</strong> [25] is a graph diffusion model tailored for scaffold hopping in drug design, which modifies the core structure of known active compounds to generate new chemical entities while preserving essential molecular features. Similarly, they are powerful tools for predicting protein structures and interactions, which is vital to understanding biological processes and designing new therapeutics. For example, models such as <strong>DiffDock</strong> [26] facilitate molecular coupling by predicting how small molecules bind to proteins.</p> <h3 id="32-materials-design">3.2 Materials Design</h3> <p>Graph diffusion models are used to design new materials with specific properties by generating graphs that represent the structures of the material, which can then be analyzed for their physical and chemical properties. By learning the graph structures of existing materials, these models can propose new materials with enhanced characteristics, such as increased strength or conductivity, which makes the methods quite relevant in fields like nanotechnology and materials science [27].</p> <h3 id="33-combinatorial-optimization">3.3 Combinatorial Optimization</h3> <p>Graph diffusion models are also applied in solving combinatorial optimization problems, where the goal is to find the best solution from a finite set of possible solutions. Models like <strong>DIFUSCO</strong> [28] can generate candidate solutions for problems like the traveling salesman and maximal independent set problem.</p> <h3 id="34-xai">3.4 xAI</h3> <p>Machine learning models are generally “black box” in nature, making them untrustworthy and unreliable. Explainable AI (xAI) refers to methods and systems that make such models explainable and interpretable. Graph Neural Networks (GNNs) are also black-box in nature, and diffusion models like <strong>D4Explainer</strong> [29] can produce both counterfactual and model-level explanations for GNNs.</p> <hr/> <h2 id="references">References</h2> <p>[1] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022.</p> <p>[2] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2021.</p> <p>[3] Video generation models as world simulators — OpenAI.</p> <p>[4] Thomas Macaulay. OpenAI’s new image generator sparks both excitement and fear, April 2022.</p> <p>[5] Kevin Roose. AI-generated art is already transforming creative work. <em>The New York Times</em>, October 2022.</p> <p>[6] OpenAI reveals Sora, a tool to make instant videos from written prompts, February 2024.</p> <p>[7] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. <em>(arXiv:1503.03585)</em>, November 2015. [arXiv:1503.03585 [cond-mat, q-bio, stat]].</p> <p>[8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In <em>Advances in Neural Information Processing Systems</em>, volume 33, page 6840–6851. Curran Associates, Inc., 2020.</p> <p>[9] Fick’s laws of diffusion, July 2024. Page Version ID: 1235588853.</p> <p>[10] Diffusion models.</p> <p>[11] Christopher M. Bishop and Hugh Bishop. <em>Deep Learning: Foundations and Concepts</em>. Springer International Publishing, Cham, 2024.</p> <p>[12] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.</p> <p>[13] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. <em>(arXiv:2209.00796)</em>, June 2024. [arXiv:2209.00796 [cs]].</p> <p>[14] Image generation with diffusion models using Keras and TensorFlow — by Vedant Jumle — Towards Data Science.</p> <p>[15] Stanley H. Chan. Tutorial on diffusion models for imaging and vision. <em>(arXiv:2403.18103)</em>, March 2024. [arXiv:2403.18103 [cs]].</p> <p>[16] Prafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis, 2021.</p> <p>[17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In <em>NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</em>, 2021.</p> <p>[18] Chengyi Liu, Wenqi Fan, Yunqing Liu, Jiatong Li, Hang Li, Hui Liu, Jiliang Tang, and Qing Li. Generative diffusion models on graphs: Methods and applications. <em>(arXiv:2302.02591)</em>, August 2023. [arXiv:2302.02591 [cs]].</p> <p>[19] Kilian Konstantin Haefeli, Karolis Martinkus, Nathanaël Perraudin, and Roger Wattenhofer. Diffusion models for graphs benefit from discrete state spaces, 2023.</p> <p>[20] Paul Erdős and Alfréd Rényi. On the evolution of random graphs. <em>Publ. Math. Inst. Hungar. Acad. Sci.</em>, 5:17–61, 1960.</p> <p>[21] Ling Yang, Zhilin Huang, Zhilong Zhang, Zhongyi Liu, Shenda Hong, Wentao Zhang, Wenming Yang, Bin Cui, and Luxia Zhang. Graphusion: Latent diffusion for graph generation. <em>IEEE Transactions on Knowledge and Data Engineering</em>, page 1–12, 2024.</p> <p>[22] Iakovos Evdaimon, Giannis Nikolentzos, Michail Chatzianastasis, Hadi Abdine, and Michalis Vazirgiannis. Neural graph generator: Feature-conditioned graph generation using latent diffusion models. <em>arXiv preprint arXiv:2403.01555</em>, 2024.</p> <p>[23] Minkai Xu, Alexander S Powers, Ron O Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In <em>International Conference on Machine Learning</em>, pages 38592–38610. PMLR, 2023.</p> <p>[24] Cong Fu, Keqiang Yan, Limei Wang, Tao Komikado, Koji Maruhashi, Kanji Uchino, Xiaoning Qian, and Shuiwang Ji. A latent diffusion model for protein structure generation.</p> <p>[25] Jos Torge, Charles Harris, Simon V. Mathis, and Pietro Lio. Diffhopp: A graph diffusion model for novel drug design via scaffold hopping, 2023.</p> <p>[26] Gabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Diffdock: Diffusion steps, twists, and turns for molecular docking, 2023.</p> <p>[27] Mengchun Zhang, Maryam Qamar, Taegoo Kang, Yuna Jung, Chenshuang Zhang, Sung-Ho Bae, and Chaoning Zhang. A survey on graph diffusion models: Generative AI in science for molecule, protein and material. <em>arXiv preprint arXiv:2304.01565</em>, 2023.</p> <p>[28] Zhiqing Sun and Yiming Yang. Diffusco: Graph-based diffusion solvers for combinatorial optimization. <em>Advances in Neural Information Processing Systems</em>, 36:3706–3731, 2023.</p> <p>[29] Jialin Chen, Shirley Wu, Abhijit Gupta, and Rex Ying. D4explainer: In-distribution explanations of graph neural network via discrete denoising diffusion. <em>Advances in Neural Information Processing Systems</em>, 36, 2024.</p>]]></content><author><name></name></author><category term="essay"/><category term="ai"/><summary type="html"><![CDATA[Study Notes]]></summary></entry><entry><title type="html">Generalization Ability as a Criteria of Intelligence</title><link href="https://bishallakha.github.io/blog/2023/general-intelligence/" rel="alternate" type="text/html" title="Generalization Ability as a Criteria of Intelligence"/><published>2023-02-05T21:01:00+00:00</published><updated>2023-02-05T21:01:00+00:00</updated><id>https://bishallakha.github.io/blog/2023/general-intelligence</id><content type="html" xml:base="https://bishallakha.github.io/blog/2023/general-intelligence/"><![CDATA[<p>Intelligence is the ability of an agent to pursue and achieve a goal: be it by imitating humans and fooling them as in the Turing test (Haugeland, 1989,6) or by understanding knowledge (Haugeland, 1989,11). However, Chollet refuses the idea of goal-specific criteria for intelligence and proposes a new definition “The intelligence of a system is a measure of its skill-acquisition efficiency over a scope of tasks, with respect to priors, experience, and generalization difficulty.” (Chollet, 2019,27). I consider Chollet’s definition of intelligence tackles vagueness and objection associated with the goal-specific definition of intelligence and provides better criteria for intelligence.</p> <p>To achieve a goal an agent needs to overcome different barriers. The number and difficulty vary according to the difficulty of the goal but still, the agent needs to go through it in order to complete that particular task. The agent can overcome the barrier only if the agent can solve the problems associated with the barrier. To solve a problem an agent needs knowledge about a problem and find reasons and logic for its solution. Once it tackles all the barriers and reaches the goal, it should also know that the task is done. Since it needs, planning, reasoning, and awareness of the situation to carry out this process, the agent can be considered intelligent. If we consider passing the Turing test, a test based on a game called ‘imitation game’ in which a computer tries to fool a human interrogator acting as a human (Haugeland, 1989,6), as a goal of an agent, the agent has barriers to understanding the language, context, and meaning and then generates a suitable answer. All these tasks require reasoning, understanding, and logic, and if the agent is able to pass the test we can consider it to have the ability to reason, understand and be logical which consequently makes it appropriate for considering it intelligent as intelligence encompasses those traits.</p> <p>The problem with goal-centric criteria is that such agents can be programmed with the ability to solve that specific task which can make all the steps taken by the agent to overcome all the barriers associated with the task mechanical, instead of intelligent. Chollet (2019,19) writes that</p> <blockquote> When a human engineer implements a chatbot by specifying answers for each possible query via if/else statements, we do not assume this chatbot to be intelligent, and we do not expect it to generalize beyond the engineer’s specifications. Likewise, if an engineer looks at a specific IQ test task, comes up with a solution, and writes down this solution in program form, we do not expect the program to generalize to new tasks, and we do not believe that the program displays intelligence – the only intelligence at work here is the engineer’s. </blockquote> <p>Even when such systems use learning-based algorithms instead of hard-coded logic they can still lack intelligence as these systems might be able to handle situations it has not handled themselves but those situations are encountered by the developer of the system. Chollet (2019,19) adds</p> <blockquote> A learning machine certainly may be intelligent: learning is a necessary condition to adapt to new information and acquire new skills. But being programmed through exposure to data is no guarantee of generalization or intelligence. Hard-coding prior knowledge into an AI is not the only way to artificially “buy” performance on the target task without inducing any generalization power. There is another way: adding more training data, which can augment skill in a specific vertical or task without affecting generalization whatsoever. </blockquote> <p>To avoid this problem Chollet proposes the generalization strength of an agent as a better criterion for intelligence. Generalization strength in this context means flexibility and adaptability to acquire new skills to tackle unseen tasks in an unseen environment. To elaborate, if we consider two agents which have a similar set of knowledge and a similar amount of past experience, the agent with higher intelligence will end up with greater skills. The generalization he is proposing is a developer-aware generalization which is the ability of a system, either learning or static, to handle situations that neither the system nor the developer of the system has encountered before (Chollet, 2019,10).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blogs/general_intelligence1-480.webp 480w, /assets/img/blogs/general_intelligence1-800.webp 800w, /assets/img/blogs/general_intelligence1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blogs/general_intelligence1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Wozniak’s coffee cup test (Generated using ChatGPT) </div> <p>In addition to that, the goal-centric criteria for intelligence can have limitations of incomparability. Intelligence can be manifested in various forms and a single set of goals might not be able to encompass all the differences. For instance, a Turing test is solely dependent on language-based abilities which can incorporate other kinds of perceptual intelligence. Along with that, a system good at one goal might not be compared to another system. How can one compare a Go-playing agent with an image-generating system? Which one is more intelligent if they can be considered intelligent? Adopting generalization ability as a criterion can address this challenge. First of all, generalization can have varying degrees enabling classification and comparison of intelligence. An agent can absolutely lack it, or have a local generalization - good at a single or well-scoped set of tasks, or have a broad generalization ability - the ability to handle a broad range of previously unencountered tasks and environments (able to pass Wozniak’s coffee cup test-without prior knowledge of the house, an agent locates the kitchen and brews a pot of coffee. Which means it locates the coffee maker, mugs, coffee, and filters. It puts a filter in the basket, adds the appropriate amount of ground, and fills the water compartment. It starts the brew cycle, waits for it to complete, and then pours it into a mug. ), or have an extreme generalization - the ability to handle the entire new task which only vaguely or abstractly resembles the previously encountered task. Secondly, this criterion is not limited by the type of input taken by the system and the output it generates freeing it from the limitations faced by the previous criterion. Lastly, it frees an agent from the necessity to pretend to be human or follow a trait of a biological system to tackle new situations and tasks- it can conjure its own approach and no matter how alien it is, if it can handle the situation then the agent can be considered intelligent.</p> <p>To sum up, I think Chollet’s objection to the goal-centric criteria of intelligence is valid as goal goal-centric approach has limitations possibility of being programmed and the inability to be comparable and distinguishable with intelligent systems consuming a wide range of input and generating a wide range of output. The generalization ability-based approach better encompasses the multidimensional aspects of intelligence and is system and task-independent making it a better alternative to goal goal-centric approach. It also presents intelligence as a spectrum instead of a binary feature enabling it to incorporate and classify various forms of intelligence. Moreover, despite defining generalization as a spectrum and thus intelligence, this approach still allows distinguishing the absolute absence of generalization and the presence of a varying degree of generalization capabilities in an agent.</p> <hr/> <h3>References</h3> <p>Haugeland, John. 1989. Artificial Intelligence: The Very Idea. MIT Press.</p> <p>Chollet, François. 2019. <a href="http://arxiv.org/abs/1911.01547">“On the Measure of Intelligence.”</a> arXiv [cs.AI]. arXiv.</p>]]></content><author><name></name></author><category term="essay"/><category term="ai"/><summary type="html"><![CDATA[Essay from Phil 497 course at BSU]]></summary></entry></feed>